{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8560292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: configuration (EDIT THESE PATHS AS NEEDED)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\" \n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "import cv2\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === EDIT THESE ===\n",
    "# default model path inferred from your OBBMetrics save_dir earlier:\n",
    "MODEL_PATH = \"/home/23ucc611/SWE/Poaching/poacher_obb/exp1/weights/best.pt\"  # change if needed\n",
    "DATA_YAML = \"/home/23ucc611/SWE/Poaching/dataset/data.yaml\"   # your uploaded file; adjust if not present. \n",
    "\n",
    "# Inputs (change per-run)\n",
    "IMAGE_PATH = \"/home/23ucc611/SWE/Poaching/dataset/test/images/0b1a3af197_jpg.rf.09fdfff449166b18d17638a9cb8aaf22.jpg\"\n",
    "IMAGE_FOLDER = \"/home/23ucc611/SWE/Poaching/dataset/test/images\"\n",
    "VIDEO_PATH = \"/path/to/input/video.mp4\"\n",
    "WEBCAM_SOURCE = 0   # 0 or 1 or RTSP URL\n",
    "\n",
    "# Output dir\n",
    "OUT_DIR = \"/home/23ucc611/SWE/Poaching/inference_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "CONF_THRES = 0.001\n",
    "DEVICE = 0  # 0 = first GPU, \"cpu\" to force CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcc9fef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classes: 72\n",
      "Loading model: /home/23ucc611/SWE/Poaching/poacher_obb/exp1/weights/best.pt\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: load classes from data.yaml (if available) and load model\n",
    "if os.path.exists(DATA_YAML):\n",
    "    with open(DATA_YAML, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    names_dict = data.get('names', {})\n",
    "    CLASS_NAMES = names_dict\n",
    "else:\n",
    "    CLASS_NAMES = None\n",
    "\n",
    "print(\"Loaded classes:\", len(CLASS_NAMES) if CLASS_NAMES else \"no data.yaml found; using indices only\")\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model:\", MODEL_PATH)\n",
    "model = YOLO(MODEL_PATH)\n",
    "print(\"Model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77634a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: helpers — extract detections, draw rotated boxes when possible, save outputs\n",
    "def angle_to_degrees(angle):\n",
    "    \"\"\"Given angle from model (try to auto-detect radians vs degrees).\"\"\"\n",
    "    # If angle magnitude > 2*pi treat as degrees already, else convert radians->deg\n",
    "    if abs(angle) > 2*math.pi:\n",
    "        return float(angle)\n",
    "    else:\n",
    "        return float(angle) * 180.0 / math.pi\n",
    "\n",
    "def polygon_from_cxcywh_angle(cx, cy, w, h, angle_deg):\n",
    "    \"\"\"Return polygon (4x2) points from center, w,h and angle in degrees (cv2.boxPoints expects (center,(w,h),angle)).\"\"\"\n",
    "    box = ((float(cx), float(cy)), (float(w), float(h)), float(angle_deg))\n",
    "    pts = cv2.boxPoints(box)  # returns float32 array shape (4,2)\n",
    "    return pts.astype(int)\n",
    "\n",
    "def draw_polygon(img, pts, color=(0,255,0), thickness=2):\n",
    "    pts = np.array(pts, dtype=np.int32).reshape((-1,1,2))\n",
    "    cv2.polylines(img, [pts], isClosed=True, color=color, thickness=thickness)\n",
    "\n",
    "def color_for_class(clsid):\n",
    "    # reproducible color per class\n",
    "    return (int((clsid*37) % 255), int((clsid*17) % 255), int((clsid*97) % 255))\n",
    "\n",
    "def extract_detections_from_result(res):\n",
    "    \"\"\"\n",
    "    Input: `res` is a single ultralytics Result (one image/frame).\n",
    "    Returns list of detection dicts:\n",
    "      { 'class_id', 'class_name', 'conf', 'xyxy': [x1,y1,x2,y2],\n",
    "        optional 'cxcywh_angle': [cx,cy,w,h,angle], optional 'polygon': [[x1,y1,...x4,y4]] }\n",
    "    \"\"\"\n",
    "    dets = []\n",
    "    boxes = getattr(res, 'boxes', None)\n",
    "    if boxes is None:\n",
    "        return dets\n",
    "\n",
    "    # prefer to read attributes if present\n",
    "    # 1) axis-aligned: boxes.xyxy\n",
    "    try:\n",
    "        xyxy = boxes.xyxy.cpu().numpy()  # shape (N,4)\n",
    "    except Exception:\n",
    "        xyxy = None\n",
    "\n",
    "    # 2) try to read rotated format: some ultralytics versions provide boxes.xywh and boxes.xywh[:,4] angle\n",
    "    # We'll attempt various fallbacks\n",
    "    has_xywh = False\n",
    "    cxcywh = None\n",
    "    if hasattr(boxes, 'xywh'):\n",
    "        try:\n",
    "            cxcywh = boxes.xywh.cpu().numpy()  # shape (N,4) or (N,5)\n",
    "            has_xywh = True\n",
    "        except Exception:\n",
    "            has_xywh = False\n",
    "\n",
    "    # 3) some OBB outputs publish boxes.data with variable columns; we'll peek at .data if others fail\n",
    "    maybe_data = None\n",
    "    try:\n",
    "        maybe_data = boxes.data.cpu().numpy()  # shape (N, M)\n",
    "    except Exception:\n",
    "        maybe_data = None\n",
    "\n",
    "    # class ids and conf\n",
    "    try:\n",
    "        confs = boxes.conf.cpu().numpy()\n",
    "        clsids = boxes.cls.cpu().numpy().astype(int)\n",
    "    except Exception:\n",
    "        # fallback\n",
    "        confs = []\n",
    "        clsids = []\n",
    "\n",
    "    N = len(confs)\n",
    "    for i in range(N):\n",
    "        d = {}\n",
    "        d['conf'] = float(confs[i])\n",
    "        d['class_id'] = int(clsids[i]) if len(clsids)>i else None\n",
    "        d['class_name'] = CLASS_NAMES[d['class_id']] if (CLASS_NAMES and d['class_id'] is not None and d['class_id'] < len(CLASS_NAMES)) else str(d['class_id'])\n",
    "\n",
    "        if xyxy is not None and len(xyxy)>i:\n",
    "            x1,y1,x2,y2 = xyxy[i].tolist()\n",
    "            d['xyxy'] = [float(x1), float(y1), float(x2), float(y2)]\n",
    "\n",
    "        # if xywh is present, try parse cx,cy,w,h,(maybe angle)\n",
    "        if has_xywh and cxcywh is not None and len(cxcywh)>i:\n",
    "            arr = cxcywh[i]\n",
    "            if arr.shape[0] >= 4:\n",
    "                cx, cy, w, h = arr[0], arr[1], arr[2], arr[3]\n",
    "                # if angle present as 5th element:\n",
    "                angle = None\n",
    "                if arr.shape[0] >= 5:\n",
    "                    angle = arr[4]\n",
    "                    angle_deg = angle_to_degrees(angle)\n",
    "                    d['cxcywh_angle'] = [float(cx), float(cy), float(w), float(h), float(angle_deg)]\n",
    "                    # polygon from rotated box\n",
    "                    d['polygon'] = polygon_from_cxcywh_angle(cx, cy, w, h, angle_deg).reshape(-1,2).tolist()\n",
    "                else:\n",
    "                    d['cxcywh'] = [float(cx), float(cy), float(w), float(h)]\n",
    "\n",
    "        # last resort: try to parse maybe_data column structure (commonly: x1,y1,x2,y2,conf,cls, maybe angle or poly coords)\n",
    "        if maybe_data is not None and len(maybe_data)>i:\n",
    "            row = maybe_data[i]\n",
    "            # try detect length: if >=9 maybe polygon coords present (cls,conf at end) - heuristic\n",
    "            # We won't overwrite existing parsed fields, only add polygon if possible\n",
    "            if 'polygon' not in d:\n",
    "                if len(row) >= 8:\n",
    "                    # attempt to detect polygon if values look like polygon (8 coords)\n",
    "                    # Heuristic: last two elements often conf and cls; if row starts with class then use different ordering - skip for safety\n",
    "                    # We'll attempt to find 8 sequential coordinates in row (normalized or absolute)\n",
    "                    # Very best-effort: find any continuous sequence of 8 float-like numbers between 0 and max(image size)\n",
    "                    vals = row.tolist()\n",
    "                    floats = [float(v) for v in vals]\n",
    "                    # try contiguous subarray of length 8 where values look like coords between 0 and 1 or >0 (we cannot know)\n",
    "                    for start in range(max(0, len(floats)-10)):\n",
    "                        sub = floats[start:start+8]\n",
    "                        if len(sub)==8:\n",
    "                            # accept this as polygon\n",
    "                            poly = [[sub[0],sub[1]],[sub[2],sub[3]],[sub[4],sub[5]],[sub[6],sub[7]]]\n",
    "                            d['polygon'] = poly\n",
    "                            break\n",
    "\n",
    "        dets.append(d)\n",
    "    return dets\n",
    "\n",
    "def annotate_image_with_detections(img_bgr, detections, conf_thres=CONF_THRES, names=CLASS_NAMES):\n",
    "    out = img_bgr.copy()\n",
    "    for d in detections:\n",
    "        if d['conf'] < conf_thres:\n",
    "            continue\n",
    "        clsid = d.get('class_id', -1)\n",
    "        label_text = f\"{d.get('class_name', clsid)}:{d['conf']:.2f}\"\n",
    "        color = color_for_class(clsid if clsid is not None else 0)\n",
    "\n",
    "        # draw polygon if available\n",
    "        if 'polygon' in d:\n",
    "            pts = np.array(d['polygon'], dtype=np.int32)\n",
    "            # if polygon coords are normalized (0..1) check and convert - assume absolute for now\n",
    "            # if coords seem within 0..1, we cannot convert without image dimensions - skipping that complexity for now\n",
    "            try:\n",
    "                draw_polygon(out, pts, color=color, thickness=2)\n",
    "                # draw label at first point\n",
    "                x,y = int(pts[0][0]), int(pts[0][1])\n",
    "                cv2.putText(out, label_text, (x, max(0,y-8)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # else fallback to xyxy\n",
    "        if 'xyxy' in d:\n",
    "            x1,y1,x2,y2 = map(int, d['xyxy'])\n",
    "            cv2.rectangle(out, (x1,y1), (x2,y2), color, 2)\n",
    "            (tw, th), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "            cv2.rectangle(out, (x1, y1 - th - 6), (x1 + tw, y1), color, -1)\n",
    "            cv2.putText(out, label_text, (x1, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "        # if only cxcywh present we can compute axis-aligned box\n",
    "        elif 'cxcywh' in d:\n",
    "            cx,cy,w,h = d['cxcywh']\n",
    "            x1 = int(cx - w/2); y1 = int(cy - h/2); x2 = int(cx + w/2); y2 = int(cy + h/2)\n",
    "            cv2.rectangle(out, (x1,y1),(x2,y2), color, 2)\n",
    "            cv2.putText(out, label_text, (x1, max(0,y1-4)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80cb05d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved annotated image: /home/23ucc611/SWE/Poaching/inference_outputs/0b1a3af197_jpg.rf.09fdfff449166b18d17638a9cb8aaf22_pred.jpg\n",
      "Saved detections CSV/JSON: /home/23ucc611/SWE/Poaching/inference_outputs/0b1a3af197_jpg.rf.09fdfff449166b18d17638a9cb8aaf22_pred.csv /home/23ucc611/SWE/Poaching/inference_outputs/0b1a3af197_jpg.rf.09fdfff449166b18d17638a9cb8aaf22_pred.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: single-image inference (runs model, extracts detections, saves annotated image + CSV/JSON)\n",
    "def infer_image(image_path, save_dir=OUT_DIR, conf_thres=CONF_THRES, device=DEVICE):\n",
    "    p = Path(image_path)\n",
    "    out_img_path = Path(save_dir) / (p.stem + \"_pred.jpg\")\n",
    "    out_csv_path = Path(save_dir) / (p.stem + \"_pred.csv\")\n",
    "    out_json_path = Path(save_dir) / (p.stem + \"_pred.json\")\n",
    "\n",
    "    results = model.predict(source=str(image_path), conf=conf_thres, device=device, verbose=False)\n",
    "    res = results[0]\n",
    "    # read image\n",
    "    img_bgr = cv2.imread(str(image_path))\n",
    "    detections = extract_detections_from_result(res)\n",
    "    # save CSV\n",
    "    df_rows = []\n",
    "    for d in detections:\n",
    "        row = {\n",
    "            \"class_id\": d.get('class_id'),\n",
    "            \"class_name\": d.get('class_name'),\n",
    "            \"conf\": d.get('conf'),\n",
    "            \"xyxy\": d.get('xyxy'),\n",
    "            \"cxcywh\": d.get('cxcywh'),\n",
    "            \"cxcywh_angle\": d.get('cxcywh_angle'),\n",
    "            \"polygon\": d.get('polygon')\n",
    "        }\n",
    "        df_rows.append(row)\n",
    "    if df_rows:\n",
    "        df = pd.DataFrame(df_rows)\n",
    "        df.to_csv(out_csv_path, index=False)\n",
    "        with open(out_json_path, 'w') as f:\n",
    "            json.dump(df_rows, f, indent=2)\n",
    "    # annotate & save\n",
    "    annotated = annotate_image_with_detections(img_bgr, detections, conf_thres=conf_thres)\n",
    "    cv2.imwrite(str(out_img_path), annotated)\n",
    "    print(\"Saved annotated image:\", out_img_path)\n",
    "    print(\"Saved detections CSV/JSON:\", out_csv_path, out_json_path)\n",
    "    return detections\n",
    "\n",
    "# Example usage:\n",
    "detections = infer_image(IMAGE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e7d7dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images found: 1796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1796/1796 [01:39<00:00, 18.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch inference finished. Annotated images: /home/23ucc611/SWE/Poaching/inference_outputs/batch\n",
      "Saved batch_detections.csv/json in /home/23ucc611/SWE/Poaching/inference_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: folder batch inference (saves annotated images + single CSV with all detections)\n",
    "def infer_folder(folder_path, save_dir=OUT_DIR, conf_thres=CONF_THRES, device=DEVICE):\n",
    "    folder = Path(folder_path)\n",
    "    out_folder = Path(save_dir) / \"batch\"\n",
    "    out_folder.mkdir(parents=True, exist_ok=True)\n",
    "    all_rows = []\n",
    "    patterns = [\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.tif\",\"*.tiff\"]\n",
    "    files = []\n",
    "    for p in patterns:\n",
    "        files += list(folder.rglob(p))\n",
    "    files = sorted(files)\n",
    "    print(\"Images found:\", len(files))\n",
    "    for img_path in tqdm(files):\n",
    "        results = model.predict(source=str(img_path), conf=conf_thres, device=device, verbose=False)\n",
    "        res = results[0]\n",
    "        detections = extract_detections_from_result(res)\n",
    "        # annotate\n",
    "        img = cv2.imread(str(img_path))\n",
    "        annotated = annotate_image_with_detections(img, detections, conf_thres=conf_thres)\n",
    "        out_img_path = out_folder / (img_path.stem + \"_pred.jpg\")\n",
    "        cv2.imwrite(str(out_img_path), annotated)\n",
    "        # collect rows\n",
    "        for d in detections:\n",
    "            row = {\n",
    "                \"image\": str(img_path),\n",
    "                \"class_id\": d.get('class_id'),\n",
    "                \"class_name\": d.get('class_name'),\n",
    "                \"conf\": d.get('conf'),\n",
    "                \"xyxy\": d.get('xyxy'),\n",
    "                \"cxcywh_angle\": d.get('cxcywh_angle'),\n",
    "                \"polygon\": d.get('polygon')\n",
    "            }\n",
    "            all_rows.append(row)\n",
    "    # save master csv/json\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df.to_csv(Path(save_dir)/\"batch_detections.csv\", index=False)\n",
    "    with open(Path(save_dir)/\"batch_detections.json\",'w') as f:\n",
    "        json.dump(all_rows, f, indent=2)\n",
    "    print(\"Batch inference finished. Annotated images:\", out_folder)\n",
    "    print(\"Saved batch_detections.csv/json in\", save_dir)\n",
    "\n",
    "# Example usage:\n",
    "infer_folder(IMAGE_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9430587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: video inference (process frames in batches -> annotate -> save output video)\n",
    "def infer_video(video_path, save_dir=OUT_DIR, conf_thres=CONF_THRES, device=DEVICE, batch_frames=8):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise FileNotFoundError(f\"Cannot open video: {video_path}\")\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out_video_path = Path(save_dir) / (Path(video_path).stem + \"_pred.mp4\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(str(out_video_path), fourcc, fps, (w,h))\n",
    "    frames = []\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "        if len(frames) >= batch_frames:\n",
    "            results = model.predict(frames, conf=conf_thres, device=device, verbose=False)\n",
    "            for fimg, res in zip(frames, results):\n",
    "                dets = extract_detections_from_result(res)\n",
    "                ann = annotate_image_with_detections(fimg, dets, conf_thres=conf_thres)\n",
    "                writer.write(ann)\n",
    "            frames = []\n",
    "    # remaining\n",
    "    if frames:\n",
    "        results = model.predict(frames, conf=conf_thres, device=device, verbose=False)\n",
    "        for fimg, res in zip(frames, results):\n",
    "            dets = extract_detections_from_result(res)\n",
    "            ann = annotate_image_with_detections(fimg, dets, conf_thres=conf_thres)\n",
    "            writer.write(ann)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    print(\"Saved annotated video to:\", out_video_path)\n",
    "\n",
    "# Example usage:\n",
    "# infer_video(VIDEO_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb8b55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: webcam / RTSP stream (display; optional saving)\n",
    "def infer_stream(source=WEBCAM_SOURCE, save=False, save_dir=OUT_DIR, conf_thres=CONF_THRES, device=DEVICE):\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Unable to open stream: {source}\")\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 20.0\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    writer = None\n",
    "    if save:\n",
    "        out_path = Path(save_dir)/(f\"stream_{Path(str(source)).stem}_pred.mp4\")\n",
    "        writer = cv2.VideoWriter(str(out_path), cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w,h))\n",
    "    print(\"Press 'q' to stop.\")\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            results = model.predict(frame, conf=conf_thres, device=device, verbose=False)\n",
    "            res = results[0]\n",
    "            dets = extract_detections_from_result(res)\n",
    "            ann = annotate_image_with_detections(frame, dets, conf_thres=conf_thres)\n",
    "            cv2.imshow(\"YOLOv8-OBB Stream\", ann)\n",
    "            if writer:\n",
    "                writer.write(ann)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "        if writer:\n",
    "            writer.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73cdc62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: simple post-processing to raise Poacher/Threat alerts (map several classes -> threat)\n",
    "THREAT_CLASS_NAMES = {\"Hunter\",\"Rifle\",\"Pistol\",\"Knife\",\"X-Bow\",\"Rope\"}  # edit to your dataset's actual class names\n",
    "\n",
    "def threats_in_detections(detections, conf_thres=CONF_THRES):\n",
    "    threats = []\n",
    "    for d in detections:\n",
    "        if d['conf'] < conf_thres:\n",
    "            continue\n",
    "        name = d.get('class_name')\n",
    "        if name in THREAT_CLASS_NAMES:\n",
    "            threats.append((name, d['conf']))\n",
    "    return threats\n",
    "\n",
    "# Example: run image inference and print alerts\n",
    "# dets = infer_image(IMAGE_PATH)\n",
    "# t = threats_in_detections(dets)\n",
    "# if t:\n",
    "#     print(\"ALERT! threats detected:\", t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf94599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
